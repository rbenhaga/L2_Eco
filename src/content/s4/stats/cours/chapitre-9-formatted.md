# Chapitre 9 : Estimation Ponctuelle

---

## ğŸ“‹ Sommaire

| NÂ° | Contenu | Page |
|----|---------|------|
| â€” | Fiche de synthÃ¨se | 239 |
| 1 | Application du principe du maximum de vraisemblance Ã  une loi binomiale | 239 |
| 2 | Application du principe du maximum de vraisemblance Ã  une densitÃ© quelconque | 246 |
| 3 | Application du principe du maximum de vraisemblance Ã  une densitÃ© quelconque avec utilisation de la fonction Gamma | 250 |
| 4 | Application du principe du maximum de vraisemblance Ã  une densitÃ© quelconque avec utilisation de la fonction Gamma | 254 |
| 5 | Application du principe du maximum de vraisemblance Ã  la loi de Bernoulli | 257 |
| 6 | Application du principe du maximum de vraisemblance Ã  la loi normale afin d'identifier un biais | 261 |

---

## ğŸ“– Introduction

Au chapitre prÃ©cÃ©dent, nous avons constatÃ© que le calcul de probabilitÃ©s Ã©tait dÃ©pendant de certains paramÃ¨tres. Par exemple, dans le cadre des calculs de probabilitÃ©s relevant de la loi normale, il est nÃ©cessaire de connaÃ®tre les paramÃ¨tres $m$ et $\sigma$ lorsque $X \sim N(m; \sigma)$.

Jusqu'ici, ces paramÃ¨tres de la population ont Ã©tÃ© considÃ©rÃ©s comme **connus**. Or en rÃ©alitÃ©, la connaissance de $m$ et $\sigma$ n'est pas systÃ©matique car les populations ont des tailles parfois trop grandes et les enquÃªtes sont trop coÃ»teuses pour pouvoir sonder une population.

> [!IMPORTANT]
> Il est donc possible de faire l'approximation de certains paramÃ¨tres en recourant aux informations relatives Ã  l'Ã©chantillon.

Par consÃ©quent, si $X \sim N(m; \sigma)$ ne peut Ãªtre utilisÃ© car $m$ et $\sigma$ sont inconnus, il faut alors chercher des **estimateurs** pour $m$ et $\sigma$, tout en vÃ©rifiant que ces estimateurs respectent certaines **propriÃ©tÃ©s dÃ©sirables**.

Dans la pratique, la moyenne arithmÃ©tique est un bon estimateur de la moyenne de la population. Cependant, certains estimateurs connus ne sont pas toujours adaptÃ©s. La **mÃ©thode du maximum de vraisemblance**, utilisÃ©e tout au long de ce chapitre, permet prÃ©cisÃ©ment de trouver des estimateurs (qui fournissent des estimations ponctuelles et non des intervalles de valeurs possibles comme nous le verrons au Chapitre 11).

### Objectif de la mÃ©thode

La mÃ©thode du maximum de vraisemblance permet de trouver la **forme mathÃ©matique d'un estimateur** grÃ¢ce aux informations relatives Ã  l'Ã©chantillon. Une fois cet estimateur obtenu, il est nÃ©cessaire de vÃ©rifier dans un second temps le respect des propriÃ©tÃ©s dÃ©sirables :

| PropriÃ©tÃ© | Description |
|-----------|-------------|
| **Sans biais** | En moyenne, l'estimateur donne la vraie valeur de la statistique recherchÃ©e |
| **EfficacitÃ©** | L'estimateur possÃ¨de une variance minimale |
| **Convergence** | L'estimateur tend vers la vraie valeur quand la taille de l'Ã©chantillon augmente |
| **ExhaustivitÃ©** | L'estimateur utilise toute l'information contenue dans l'Ã©chantillon |

### Intuition de la mÃ©thode

La mÃ©thode est intuitive. Elle consiste Ã  trouver un (ou plusieurs) estimateur(s) tel que la probabilitÃ© (la vraisemblance) d'observer l'Ã©chantillon *a posteriori* est **maximale**. Ainsi, l'Ã©chantillon doit fournir la meilleure information possible.

**Maximiser la vraisemblance** consiste Ã  maximiser le produit des densitÃ©s ou des probabilitÃ©s individuelles, i.e., la probabilitÃ© d'obtenir l'image $x_1$ Â« et Â» l'image $x_2$, Â« et Â» l'image $x_n$.

La technique exige donc la connaissance de la **loi de probabilitÃ©** suivie par $X$ afin de procÃ©der Ã  la maximisation de la vraisemblance. Les lois de probabilitÃ©s peuvent Ãªtre de plusieurs types : **continues** ou **discrÃ¨tes**.

---

## ğŸ“ Fiche de SynthÃ¨se

### Le maximum de vraisemblance : les Ã©tapes de la mÃ©thode

#### Ã‰tape 1 : La fonction de vraisemblance

Afin de trouver un estimateur de $\theta$, paramÃ¨tre inconnu d'une loi de probabilitÃ©, la fonction de vraisemblance est construite en prenant :

**Cas continu** (produit des densitÃ©s de probabilitÃ©s) :
$$L(x_1, \ldots, x_n; \theta) = f(x_1) \times \cdots \times f(x_n)$$

**Cas discret** (produit des probabilitÃ©s individuelles) :
$$L(x_1, \ldots, x_n; \theta) = P(X = x_1) \times \cdots \times P(X = x_n)$$

#### Ã‰tape 2 : La fonction de Log-vraisemblance

La fonction de vraisemblance en logarithme est ensuite dÃ©duite afin de **linÃ©ariser** l'expression prÃ©cÃ©dente :

$$\ln L(x_1, \ldots, x_n; \theta) = \sum_{i=1}^{n} \ln f(x_i)$$

ou

$$\ln L(x_1, \ldots, x_n; \theta) = \sum_{i=1}^{n} \ln P(X = x_i)$$

#### Ã‰tape 3 : Maximisation

La fonction de Log-vraisemblance est ensuite maximisÃ©e de maniÃ¨re Ã  trouver l'estimateur $\hat{\theta}$.

**Condition de premier ordre :**
$$\frac{\partial \ln L(x_1, \ldots, x_n; \theta)}{\partial \theta} = 0$$

**Condition de second ordre :**
$$\frac{\partial^2 \ln L(x_1, \ldots, x_n; \theta)}{\partial \theta^2} \leq 0$$

---

## ğŸ“˜ Exercice 1 : Application du principe du maximum de vraisemblance Ã  une loi binomiale

### Ã‰noncÃ©

On souhaite Ã©tudier les entreprises franÃ§aises dont l'effectif est de 10 salariÃ©s. En particulier, on s'intÃ©resse aux personnes gagnant plus de 3 000 euros nets/mois. Pour cela :

1. Quelles lois proposez-vous pour les variables alÃ©atoires $X_i$ et $K$ ? Justifier.
2. On Ã©tudie $m$ entreprises de 10 salariÃ©s pour lesquelles on dispose d'un Ã©chantillon empirique de la variable $K$ : $(k_1, \ldots, k_m)$. DÃ©terminez le(s) estimateur(s) du maximum de vraisemblance du (ou des) paramÃ¨tre(s) de la loi de $K$.
3. DÃ©montrer les propriÃ©tÃ©s du (ou des) estimateur(s).

---

### Question 1 : Quelles lois proposez-vous pour les variables alÃ©atoires $X_i$ et $K$ ?

#### Loi de $X_i$

- **Ã‰preuve alÃ©atoire** : Â« prendre un salariÃ© au hasard dans une entreprise de 10 salariÃ©s Â».
- **Variable alÃ©atoire $X$** : Â« salaire en euros nets/mois Â».

| Valeur | Ã‰vÃ©nement | ProbabilitÃ© |
|--------|-----------|-------------|
| $x_i = 1$ | $(A)$ : Â« le salariÃ© gagne au moins 3 000 euros nets/mois Â» | $p = P(A)$ |
| $x_i = 0$ | $(\bar{A})$ : Â« le salariÃ© gagne moins de 3 000 euros nets/mois Â» | $(1 - p) = P(\bar{A}) = q$ |

On a **2 Ã©vÃ©nements mutuellement exclusifs** et un seul tirage (une seule Ã©preuve), d'oÃ¹ :

> [!NOTE]
> $$X_i \sim B(1; p) \quad \text{avec} \quad P(X_i = x_i) = p^{x_i} (1 - p)^{1-x_i}$$

#### Loi de $K$

Si on a $n = 10$ variables alÃ©atoires indÃ©pendantes de Bernoulli, alors la somme donne une variable alÃ©atoire suivant une **loi binomiale** :

$$K \sim B(10; p) \quad \text{avec} \quad P(K = k) = C_{10}^k \cdot p^k \cdot (1 - p)^{10-k}$$

---

### Question 2 : DÃ©termination de l'estimateur du maximum de vraisemblance

#### Pourquoi utiliser la mÃ©thode du maximum de vraisemblance ?

Si on souhaite calculer une probabilitÃ© pour un Ã©vÃ©nement $k_i$, il est nÃ©cessaire de connaÃ®tre la valeur de $p$. Il faut donc une **forme analytique** pour $p$, i.e., un estimateur que l'on notera $\hat{p}$. Les calculs effectuÃ©s sur l'Ã©chantillon doivent donner une **estimation ponctuelle** de $p$.

> [!TIP]
> **Pourquoi Â« estimation ponctuelle Â» ?**
> 
> Pour calculer une probabilitÃ© nous avons besoin d'une estimation issue de $\hat{p}$. Dans le chapitre suivant, nous verrons qu'il est possible d'estimer un paramÃ¨tre en lui donnant un intervalle. Notons qu'il n'est pas possible de munir $\hat{p}$ d'un intervalle de valeurs, ceci impliquerait une infinitÃ© de probabilitÃ©s estimÃ©es avec $P(K = k_i)$.

#### Construction de la fonction de vraisemblance

L'estimateur issu de la mÃ©thode du maximum de vraisemblance (EMV) de $p$ est basÃ© sur le principe suivant : on va chercher l'estimateur $\hat{p}$ qui rend la fonction de vraisemblance **maximale** (c'est-Ã -dire qui rend la probabilitÃ© d'apparition de l'Ã©chantillon observÃ© *a posteriori* maximale).

La fonction de vraisemblance $L(\cdot)$ se construit par le **produit des probabilitÃ©s individuelles** :

$$L(k_1, \ldots, k_m; p) = P(k_1) \times \cdots \times P(k_m) = \prod_{i=1}^{m} C_{10}^{k_i} \cdot p^{k_i} \cdot (1 - p)^{10-k_i}$$

> [!NOTE]
> **Remarques :**
> - La fonction de vraisemblance peut s'Ã©crire $L(k_1, \ldots, k_m; p)$ ou $L(\cdot)$ pour raccourcir la notation.
> - La fonction de vraisemblance est dÃ©finie sur un **Ã©chantillon empirique** $(k_1, k_2, \ldots, k_m)$, puisque nous devons rendre la probabilitÃ© d'apparition de l'Ã©chantillon observÃ© *a posteriori* maximale.
> - Les propriÃ©tÃ©s liÃ©es aux estimateurs seront au contraire dÃ©montrÃ©es dans le cadre d'**Ã©chantillons thÃ©oriques**.

#### Passage Ã  la Log-vraisemblance

Pour maximiser la vraisemblance, la fonction Log-vraisemblance $(\ln L)$ est retenue car le logarithme permet de **linÃ©ariser un produit** (il s'agit aussi d'une transformation monotone et croissante laissant l'extremum inchangÃ©).

$$\ln L(k_1, \ldots, k_m; p) = \sum_{i=1}^{m} \left[ \ln C_{10}^{k_i} + k_i \ln p + (10 - k_i) \ln (1 - p) \right]$$

> [!TIP]
> Le passage en logarithme n'est pas nÃ©cessaire si la forme mathÃ©matique de $L(\cdot)$ est suffisamment simple pour trouver instantanÃ©ment $\hat{p}$. Dans la pratique, il est quasiment impÃ©ratif de passer en logarithme afin de travailler avec l'opÃ©rateur **somme** qui est plus facile Ã  manier que l'opÃ©rateur **produit**.

#### Calcul de la dÃ©rivÃ©e

La dÃ©rivÃ©e par rapport Ã  $p$ est :

$$\frac{\partial \ln L(\cdot)}{\partial p} = \sum_{i=1}^{m} \left[ \frac{k_i}{p} - \frac{10 - k_i}{1 - p} \right]$$

#### Ã‰quation du maximum de vraisemblance

$$\frac{\partial \ln L(\cdot)}{\partial p} = 0 \Rightarrow \sum_{i=1}^{m} \left[ \frac{k_i}{\hat{p}} - \frac{10 - k_i}{1 - \hat{p}} \right] = 0$$

D'oÃ¹ :
$$\frac{\sum_{i=1}^{m} k_i}{\hat{p}} - \frac{\sum_{i=1}^{m} (10 - k_i)}{1 - \hat{p}} = 0$$

On trouve :
$$\sum_{i=1}^{m} k_i - m \cdot 10 \cdot \hat{p} = 0$$

La solution existe, donc l'estimateur s'Ã©crit :

> [!IMPORTANT]
> $$\boxed{\hat{p}(k_1, \ldots, k_m) = \frac{\sum_{i=1}^{m} k_i}{10m} = \frac{\bar{k}}{10}}$$

Ce rÃ©sultat signifie que pour calculer la probabilitÃ© $P(K = k) = C_{10}^k \cdot p^k \cdot (1 - p)^{10-k}$ dont la seule inconnue est $p$, il faut remplacer $p$ par son estimateur $\hat{p}$.

#### VÃ©rification des conditions de second ordre

Le rÃ©sultat que nous avons trouvÃ© correspond Ã  un maximum si les conditions de second ordre sont vÃ©rifiÃ©es :

$$\frac{\partial^2 \ln L(\cdot)}{\partial p^2} = - \sum_{i=1}^{m} \left[ \frac{k_i}{p^2} + \frac{10 - k_i}{(1 - p)^2} \right]$$

Par dÃ©finition, $10 \geq k_i$. Par consÃ©quent :

$$\frac{\partial^2 \ln L(k_1, \ldots, k_m; p)}{\partial p^2} = - \sum_{i=1}^{m} \left[ \frac{k_i}{\hat{p}^2} + \frac{10 - k_i}{(1 - \hat{p})^2} \right] \leq 0$$

âœ… La dÃ©rivÃ©e seconde est nÃ©gative, nous sommes bien en prÃ©sence d'un **maximum**.

Ainsi, l'estimateur relatif Ã  $p$ sur l'**Ã©chantillon thÃ©orique** s'Ã©crit :

$$\hat{p}(K_1, \ldots, K_m) = \frac{\sum_{i=1}^{m} K_i}{10m} = \frac{\bar{K}}{10}$$

---

### Question 3 : DÃ©monstration des propriÃ©tÃ©s de l'estimateur

Soit un Ã©chantillon thÃ©orique alÃ©atoire de taille $m$, $(K_1, K_2, \ldots, K_m)$. Nous travaillons dÃ©sormais avec $(K_1, K_2, \ldots, K_m)$ et non plus avec $(k_1, k_2, \ldots, k_m)$ de maniÃ¨re Ã  montrer que les propriÃ©tÃ©s de l'estimateur $\hat{p}(K_1, \ldots, K_m)$ sont valables quel que soit l'Ã©chantillon.

> [!NOTE]
> **HypothÃ¨se fondamentale :**
> 
> Les $K_i$ sont supposÃ©s **indÃ©pendants et identiquement distribuÃ©s (i.i.d.)**. Cette hypothÃ¨se sera systÃ©matiquement utilisÃ©e.

Si les $K_i$ sont identiquement distribuÃ©s, cela signifie que pour tout $i = 1, \ldots, m$ : $K_i \equiv K$, donc :

- $E(K_i) = E(K) = 10p$ [puisque $K \sim B(10; p)$ : l'espÃ©rance d'une v.a. suivant une loi binomiale est $np$]
- $V(K_i) = V(K) = 10p(1 - p)$ [la variance d'une v.a. suivant une loi binomiale est $npq$]

---

#### PropriÃ©tÃ© 1 : $\hat{p}(K_1, \ldots, K_m)$ est un estimateur sans biais (ESB)

L'estimateur $\hat{p}$ est **sans biais** si son espÃ©rance est Ã©gale Ã  sa vraie valeur, c'est-Ã -dire si :

$$E[\hat{p}(K_1, \ldots, K_m)] = p$$

**DÃ©monstration :**

$$E[\hat{p}(K_1, \ldots, K_m)] = E\left[ \frac{\sum_{i=1}^{m} K_i}{10m} \right] = \frac{1}{10m} \sum_{i=1}^{m} E[K_i] = \frac{1}{10m} \cdot m \cdot 10p = p$$

> [!IMPORTANT]
> $$E[\hat{p}(K_1, \ldots, K_m)] = p$$
> 
> Donc $\hat{p}(K_1, \ldots, K_m)$ est un **estimateur sans biais**.

**InterprÃ©tation :** En prenant plusieurs Ã©chantillons et en calculant autant de fois $\hat{p}(K_1, \ldots, K_m)$, la moyenne de ces estimations donne le Â« vrai Â» paramÃ¨tre $p$ (celui qui serait estimÃ© sur l'ensemble de la population). C'est la premiÃ¨re exigence que l'on se fixe : obtenir en moyenne le vrai paramÃ¨tre.

Cependant la moyenne n'a de signification que si la variance est faible. Ce qui nous conduit Ã  examiner la seconde propriÃ©tÃ©.

---

#### PropriÃ©tÃ© 2 : $\hat{p}(K_1, \ldots, K_m)$ est un estimateur convergent

$\hat{p}(K_1, \ldots, K_m)$ est un estimateur **convergent** s'il converge en limite de probabilitÃ© vers sa vraie valeur :

$$\lim_{m \to \infty} P(\hat{p} = p) = 1$$

Cela signifie que :

$$\forall \varepsilon > 0, \exists \eta > 0 \text{ tel que } P(|\hat{p} - p| > \varepsilon) < \eta$$

Autrement dit, lorsque la taille de l'Ã©chantillon est grande, la probabilitÃ© pour que l'estimateur s'Ã©loigne de sa vraie valeur est nulle mais infinitÃ©simale.

**DÃ©monstration :**

Comme $\hat{p}$ est un ESB, montrer qu'il est convergent revient Ã  montrer que lorsque la taille de l'Ã©chantillon augmente, la variance de l'estimateur tend vers 0, soit :

$$\lim_{m \to \infty} V(\hat{p}(K_1, \ldots, K_m)) = 0$$

On a :

$$V[\hat{p}(K_1, \ldots, K_m)] = V\left[ \frac{\sum_{i=1}^{m} K_i}{10m} \right] = \frac{1}{(10m)^2} \sum_{i=1}^{m} V[K_i] = \frac{1}{100m^2} \cdot m \cdot 10p(1 - p) = \frac{pq}{10m}$$

On a bien :

$$\lim_{m \to \infty} V(\hat{p}(K_1, \ldots, K_m)) = 0$$

> [!IMPORTANT]
> $\hat{p}(K_1, \ldots, K_m)$ est un **estimateur convergent**. Comme il est sans biais, on dit qu'il est **absolument convergent**.

On pourrait s'arrÃªter ici et se contenter d'avoir un estimateur absolument convergent. En effet si la variance tend vers 0, cela signifie qu'Ã  chaque fois que l'on prend au hasard un Ã©chantillon assez grand, on est pratiquement sÃ»r de ne pas s'Ã©loigner de la vraie valeur $p$.

Il est nÃ©anmoins possible d'Ãªtre un peu plus exigeant en se demandant si la variance (mesurant la dispersion entre l'estimateur et sa vraie valeur) est bien la plus faible possible. Il s'agit de la troisiÃ¨me propriÃ©tÃ© : **l'efficacitÃ©**.

---

#### PropriÃ©tÃ© 3 : L'estimateur $\hat{p}(K_1, \ldots, K_m)$ est efficace

$\hat{p}(K_1, \ldots, K_m)$ est **efficace** s'il est un estimateur sans biais possÃ©dant une **variance minimale**.

L'**inÃ©galitÃ© de FrÃ©chet-Rao-CramÃ©r-Darmois (FRCD)** indique que tout estimateur sans biais vÃ©rifie :

$$V(\hat{p}) \geq \frac{1}{I(p)}$$

avec $I(p)$ la **quantitÃ© d'information de Fisher** :

$$I(p) = - E\left[ \frac{\partial^2 \ln L(\cdot)}{\partial p^2} \right]$$

La variance de l'estimateur est minimale si celle-ci est Ã©gale Ã  la **borne infÃ©rieure** de l'inÃ©galitÃ© de FRCD, i.e. :

$$V(\hat{p}) = \frac{1}{I(p)}$$

**DÃ©monstration :**

La quantitÃ© d'information de Fisher est donnÃ©e par :

$$I(p) = - E\left[ \frac{\partial^2 \ln L(\cdot)}{\partial p^2} \right] = - E\left[ - \sum_{i=1}^{m} \frac{K_i}{p^2} + \sum_{i=1}^{m} \frac{10 - K_i}{(1 - p)^2} \right]$$

$$= \sum_{i=1}^{m} \frac{E[K_i]}{p^2} + \sum_{i=1}^{m} \frac{E[10 - K_i]}{(1 - p)^2}$$

$$= \frac{1}{p^2} \sum_{i=1}^{m} E[K_i] + \frac{1}{(1 - p)^2} \sum_{i=1}^{m} E[10 - K_i]$$

$$= \frac{10mp}{p^2} + \frac{10m(1 - p)}{(1 - p)^2}$$

$$= \frac{10m}{p(1 - p)}$$

Et donc :

$$\frac{1}{I(p)} = \frac{p(1 - p)}{10m}$$

Or, nous avons vu que :

$$V(\hat{p}) = \frac{p(1 - p)}{10m}$$

Donc :

$$V(\hat{p}) = \frac{1}{I(p)}$$

> [!IMPORTANT]
> L'estimateur $\hat{p}$ est **efficace**.

---

#### Conclusion sur l'exercice 1

> [!TIP]
> **RÃ©sultat final :**
> 
> L'estimateur $\hat{p}(K_1, \ldots, K_m) = \frac{\sum_{i=1}^{m} K_i}{10m}$ est un estimateur :
> - âœ… **Sans biais**
> - âœ… **Convergent** (absolument convergent)
> - âœ… **Efficace**

---

## ğŸ“˜ Exercice 2 : Application du principe du maximum de vraisemblance Ã  une densitÃ© quelconque

### Ã‰noncÃ©

Soit $(x_1, \ldots, x_n)$ un Ã©chantillon alÃ©atoire de taille $n$ provenant d'une variable alÃ©atoire de densitÃ© $f(x; \theta)$ telle que :

$$f(x; \theta) = \frac{1}{\theta} e^{-\frac{x}{\theta}}; \quad x > 0 \text{ et } \theta > 0$$

1. En dÃ©taillant votre dÃ©marche, dÃ©terminez l'estimateur du maximum de vraisemblance.
2. DÃ©montrez les propriÃ©tÃ©s de l'estimateur obtenu.

---

### Question 1 : DÃ©termination de l'EMV

L'EMV de $\theta$, seul paramÃ¨tre de la loi de $X$ Ã  estimer, est basÃ© sur le principe suivant : on cherche l'estimateur $\hat{\theta}$ qui rend la fonction de vraisemblance **maximale**.

#### Fonction de vraisemblance

La fonction de vraisemblance se construit par le produit des densitÃ©s de probabilitÃ©s :

$$L(x_1, \ldots, x_n; \theta) = f(x_1) \times \cdots \times f(x_n)$$

#### Fonction de Log-vraisemblance

$$\ln L(x_1, \ldots, x_n; \theta) = -n \ln \theta - \sum_{i=1}^n \frac{x_i}{\theta}$$

#### Condition de premier ordre

$$\frac{\partial \ln L(x_1, \ldots, x_n; \theta)}{\partial \theta} = 0$$

On a :

$$\frac{\partial \ln L(x_1, \ldots, x_n; \theta)}{\partial \theta} = -\frac{n}{\theta} + \sum_{i=1}^n \frac{x_i}{\theta^2} = 0$$

On obtient :

> [!IMPORTANT]
> $$\boxed{\hat{\theta} = \frac{\sum_{i=1}^n x_i}{n} = \bar{x}}$$

Donc $\hat{\theta}(x_1, \ldots, x_n) = \bar{x}$. On suppose que les conditions de second ordre sont remplies :

$$\frac{\partial^2 \ln L(x_1, \ldots, x_n; \theta)}{\partial \theta^2} \leq 0$$

---

### Question 2 : PropriÃ©tÃ©s de l'estimateur

Soit un Ã©chantillon alÃ©atoire de taille $n$, $(X_1, \ldots, X_n)$, associÃ© Ã  la densitÃ© de probabilitÃ© $f(x; \theta)$. Nous supposons que les $X_i$ sont **indÃ©pendantes et identiquement distribuÃ©es (i.i.d)**.

Cela implique que pour tout $i = 1, \ldots, n$ : $X_i \equiv X$ oÃ¹ :
- $E(X_i) = E(X)$ et $V(X_i) = V(X)$
- $X_i$ et $X_j$ sont indÃ©pendantes pour tout $i \neq j$

---

#### PropriÃ©tÃ© 1 : L'estimateur $\hat{\theta}$ est sans biais

$\hat{\theta}(X_1, \ldots, X_n)$ est sans biais si son espÃ©rance est Ã©gale Ã  sa vraie valeur : $E[\hat{\theta}(X_1, \ldots, X_n)] = \theta$.

**DÃ©monstration :**

$$E[\hat{\theta}(X_1, \ldots, X_n)] = E\left[\frac{1}{n} \sum_{i=1}^n X_i\right] = \frac{1}{n} \sum_{i=1}^n E(X_i) = \frac{1}{n} \cdot n E(X) = E(X)$$

Or, nous avons vu au Chapitre 3 que :

$$E(X) = \int_0^{\infty} x f(x) dx = \int_0^{\infty} x \frac{1}{\theta} e^{-\frac{x}{\theta}} dx = \theta \Gamma(2) = \theta \cdot 1! = \theta$$

> [!IMPORTANT]
> Donc $E[\hat{\theta}(X_1, \ldots, X_n)] = \theta$.
> 
> $\hat{\theta}(X_1, \ldots, X_n)$ est un **estimateur sans biais**.

---

#### PropriÃ©tÃ© 2 : L'estimateur $\hat{\theta}$ est efficace

$\hat{\theta}(X_1, \ldots, X_n)$ est un estimateur efficace s'il est sans biais et s'il possÃ¨de une **variance minimale**.

D'aprÃ¨s l'inÃ©galitÃ© de **FrÃ©chet-Rao-CramÃ©r-Darmois**, tout estimateur sans biais vÃ©rifie :

$$V(\hat{\theta}) \geq \frac{1}{I(\theta)}$$

avec $I(\theta)$ la quantitÃ© d'information de Fisher :

$$I(\theta) = -E\left[\frac{\partial^2 \ln L(\cdot)}{\partial \theta^2}\right]$$

La variance de l'estimateur sera minimale si :

$$V(\hat{\theta}) = \frac{1}{I(\theta)}$$

**Calcul de $V(\hat{\theta})$ :**

$$V(\hat{\theta}(X_1, \ldots, X_n)) = V\left[\frac{1}{n} \sum_{i=1}^n X_i\right] = \frac{1}{n^2} \sum_{i=1}^n V(X_i) = \frac{1}{n^2} \cdot n V(X) = \frac{V(X)}{n}$$

Or, nous savons que $V(X) = E(X^2) - [E(X)]^2$, donc :

$$E(X^2) = \int_0^{\infty} x^2 \frac{1}{\theta} e^{-\frac{x}{\theta}} dx = \theta^2 \Gamma(3) = 2 \theta^2$$

Donc :

$$V(X) = E(X^2) - [E(X)]^2 = 2 \theta^2 - \theta^2 = \theta^2$$

On en dÃ©duit alors :

$$V(\hat{\theta}(X_1, \ldots, X_n)) = \frac{1}{n} V(X) = \frac{\theta^2}{n}$$

**Calcul de $I(\theta)$ :**

$$I(\theta) = -E\left[\frac{\partial^2 \ln L(\cdot)}{\partial \theta^2}\right] = -E\left[\frac{n}{\theta^2} - 2 \frac{\sum_{i=1}^n X_i}{\theta^3}\right] = -\frac{n}{\theta^2} + 2 \frac{n \theta}{\theta^3} = \frac{n}{\theta^2}$$

**VÃ©rification :**

$$V(\hat{\theta}(X_1, \ldots, X_n)) = \frac{\theta^2}{n} = \frac{1}{I(\theta)}$$

> [!IMPORTANT]
> L'estimateur $\hat{\theta}(X_1, \ldots, X_n)$ est donc **efficace**, car sa variance est minimale.

---

#### PropriÃ©tÃ© 3 : $\hat{\theta}$ est un estimateur convergent

$\hat{\theta}(X_1, \ldots, X_n)$ est un estimateur convergent si :

$$\lim_{n \to \infty} P(|\hat{\theta} - \theta| > \epsilon) = 0$$

Autrement dit :

$$\forall \eta > 0, \exists \eta > 0 \text{ et } \exists \epsilon > 0 \text{ tels que } P(|\hat{\theta} - \theta| > \epsilon) < \eta$$

---

## ğŸ“˜ Exercice 4 : Application du principe du maximum de vraisemblance Ã  une densitÃ© quelconque avec utilisation de la fonction Gamma

### Ã‰noncÃ©

Soit $(x_1, \ldots, x_n)$ un Ã©chantillon alÃ©atoire de taille $n$ provenant d'une variable alÃ©atoire $X$ de densitÃ© $f(x, \theta)$ suivante :

$$f(x ; \theta) = \frac{2}{\theta} x e^{-\frac{x^2}{\theta}} ; \quad x > 0 \text{ et } \theta > 0$$

1. En dÃ©taillant votre dÃ©marche, dÃ©terminez l'estimateur du maximum de vraisemblance.
2. DÃ©montrez les propriÃ©tÃ©s de l'estimateur obtenu.

---

### Question 1 : DÃ©termination de l'EMV

L'EMV de $\theta$, seul paramÃ¨tre de la loi de $X$ Ã  estimer, est basÃ© sur le principe suivant : on cherche l'estimateur $\hat{\theta}$ qui rend la fonction de vraisemblance **maximale**.

#### Fonction de vraisemblance

$$L(x_1, \ldots, x_n ; \theta) = f(x_1) \times \cdots \times f(x_n)$$

#### Fonction de Log-vraisemblance

$$\ln L(x_1, \ldots, x_n ; \theta) = n \ln 2 - n \ln \theta + \sum_{i=1}^n \ln x_i - \sum_{i=1}^n \frac{x_i^2}{\theta}$$

#### Condition de premier ordre

$$\frac{\partial \ln L(x_1, \ldots, x_n ; \theta)}{\partial \theta} = 0$$

On a :

$$\frac{\partial \ln L(x_1, \ldots, x_n ; \theta)}{\partial \theta} = -\frac{n}{\theta} + \sum_{i=1}^n \frac{x_i^2}{\theta^2} = 0$$

On obtient :

> [!IMPORTANT]
> $$\boxed{\hat{\theta} = \frac{\sum_{i=1}^n x_i^2}{n} = \overline{x^2}}$$

Donc $\hat{\theta}(x_1, \ldots, x_n) = \overline{x^2}$. On suppose que les conditions de second ordre sont remplies :

$$\frac{\partial^2 \ln L(x_1, \ldots, x_n ; \theta)}{\partial \theta^2} \leq 0$$

---

### Question 2 : PropriÃ©tÃ©s de l'estimateur

Soit un Ã©chantillon thÃ©orique alÃ©atoire de taille $n$, $(X_1, \ldots, X_n)$, associÃ© Ã  la densitÃ© de probabilitÃ© $f(x ; \theta)$. Nous supposons que les $X_i$ sont **indÃ©pendantes et identiquement distribuÃ©es (i.i.d)**.

Cela implique que pour tout $i = 1, \ldots, n$ : $X_i \equiv X$ oÃ¹ :
- $E(X_i) = E(X)$ et $V(X_i) = V(X)$
- $X_i$ et $X_j$ sont indÃ©pendantes pour tout $i \neq j$

---

#### PropriÃ©tÃ© 1 : L'estimateur $\hat{\theta}$ est sans biais

$\hat{\theta}(X_1, \ldots, X_n)$ est sans biais si son espÃ©rance est Ã©gale Ã  sa vraie valeur : $E[\hat{\theta}(X_1, \ldots, X_n)] = \theta$.

**DÃ©monstration :**

$$E[\hat{\theta}(X_1, \ldots, X_n)] = E\left[\frac{1}{n} \sum_{i=1}^n X_i^2\right] = \frac{1}{n} \sum_{i=1}^n E(X_i^2) = \frac{1}{n} \cdot n E(X^2) = E(X^2)$$

Or, nous avons vu au Chapitre 3 que :

$$E(X^2) = \int_0^{\infty} x^2 f(x) dx = \int_0^{\infty} x^2 \frac{2}{\theta} x e^{-\frac{x^2}{\theta}} dx = \theta \Gamma(2) = \theta \cdot 1! = \theta$$

> [!IMPORTANT]
> Donc $E[\hat{\theta}(X_1, \ldots, X_n)] = \theta$.
> 
> $\hat{\theta}(X_1, \ldots, X_n)$ est un **estimateur sans biais**.

---

#### PropriÃ©tÃ© 2 : L'estimateur $\hat{\theta}$ est efficace

$\hat{\theta}(X_1, \ldots, X_n)$ est un estimateur efficace s'il est sans biais et s'il possÃ¨de une **variance minimale**.

D'aprÃ¨s l'inÃ©galitÃ© de **FrÃ©chet-Rao-CramÃ©r-Darmois**, tout estimateur sans biais vÃ©rifie :

$$V(\hat{\theta}) \geq \frac{1}{I(\theta)}$$

avec $I(\theta)$ la quantitÃ© d'information de Fisher :

$$I(\theta) = -E\left[\frac{\partial^2 \ln L(\cdot)}{\partial \theta^2}\right]$$

La variance de l'estimateur sera minimale si :

$$V(\hat{\theta}) = \frac{1}{I(\theta)}$$

**Calcul de $V(\hat{\theta})$ :**

$$V(\hat{\theta}(X_1, \ldots, X_n)) = V\left[\frac{1}{n} \sum_{i=1}^n X_i^2\right] = \frac{1}{n^2} \sum_{i=1}^n V(X_i^2) = \frac{1}{n^2} \cdot n V(X^2) = \frac{V(X^2)}{n}$$

Or, nous savons que $V(X^2) = E(X^4) - [E(X^2)]^2$, donc :

$$E(X^4) = \int_0^{\infty} x^4 \frac{2}{\theta} x e^{-\frac{x^2}{\theta}} dx = \theta^2 \Gamma(3) = 2 \theta^2$$

Donc :

$$V(X^2) = E(X^4) - [E(X^2)]^2 = 2 \theta^2 - \theta^2 = \theta^2$$

On en dÃ©duit alors :

$$V(\hat{\theta}(X_1, \ldots, X_n)) = \frac{1}{n} V(X^2) = \frac{\theta^2}{n}$$

**Calcul de $I(\theta)$ :**

$$I(\theta) = -E\left[\frac{\partial^2 \ln L(\cdot)}{\partial \theta^2}\right] = -E\left[\frac{n}{\theta^2} - 2 \frac{\sum_{i=1}^n X_i^2}{\theta^3}\right] = -\frac{n}{\theta^2} + 2 \frac{n \theta}{\theta^3} = \frac{n}{\theta^2}$$

**VÃ©rification :**

$$V(\hat{\theta}(X_1, \ldots, X_n)) = \frac{\theta^2}{n} = \frac{1}{I(\theta)}$$

> [!IMPORTANT]
> L'estimateur $\hat{\theta}(X_1, \ldots, X_n)$ est donc **efficace**, car sa variance est minimale.

---

#### PropriÃ©tÃ© 3 : $\hat{\theta}$ est un estimateur convergent

$\hat{\theta}(X_1, \ldots, X_n)$ est un estimateur convergent s'il converge en limite de probabilitÃ© vers sa vraie valeur :

$$\lim_{n \to \infty} P(\hat{\theta} = \theta) = 1$$

Cela signifie que :

$$\forall \epsilon > 0, \exists \eta > 0 \text{ tel que } P(|\hat{\theta} - \theta| > \epsilon) < \eta$$

**DÃ©monstration :**

Comme $\hat{\theta}$ est un ESB, montrer qu'il est convergent revient Ã  montrer que lorsque la taille de l'Ã©chantillon augmente, la variance de l'estimateur tend vers 0, soit :

$$\lim_{n \to \infty} V(\hat{\theta}(X_1, \ldots, X_n)) = 0$$

On a :

$$\lim_{n \to \infty} V(\hat{\theta}(X_1, \ldots, X_n)) = \lim_{n \to \infty} \frac{\theta^2}{n} = 0$$

> [!IMPORTANT]
> L'estimateur est donc **convergent**. De plus, comme il est sans biais, il est **absolument convergent**.

---

## ğŸ“˜ Exercice 5 : Application du principe du maximum de vraisemblance Ã  la loi de Bernoulli

### Ã‰noncÃ©

Un institut de sondage se propose d'Ã©tudier la population franÃ§aise au mois de janvier 2005. Il dÃ©finit pour cela la variable alÃ©atoire suivante :

$X_i$ prend la valeur **1** si le i-Ã¨me individu de la population franÃ§aise est atteint par la grippe, et **0** sinon.

1. Quelle loi proposez-vous pour $X_i$ ? Justifiez votre choix.
2. Soit un Ã©chantillon $(x_1, \ldots, x_n)$ provenant de la loi de $X_i$. DÃ©terminez l'estimateur du maximum de vraisemblance du paramÃ¨tre issu de la loi de $X_i$.
3. DÃ©montrez les propriÃ©tÃ©s de cet estimateur.

---

### Question 1 : Quelle loi proposez-vous pour $X_i$ ?

- **Ã‰preuve alÃ©atoire** : Â« prendre un individu au hasard Â».
- **Variable alÃ©atoire $X_i$** : Â« Ã©tat de l'individu i Â».

De l'Ã©preuve alÃ©atoire, on dÃ©duit deux Ã©vÃ©nements :

| Valeur | Ã‰vÃ©nement | ProbabilitÃ© |
|--------|-----------|-------------|
| $x_i = 1$ | $(A)$ : Â« l'individu est malade Â» | $p = P(A)$ |
| $x_i = 0$ | $(\bar{A})$ : Â« l'individu est sain Â» | $(1 - p) = q = P(\bar{A})$ |

L'ensemble des possibles : $X(\Omega) = \{0 ; 1\}$.

Il y a **2 probabilitÃ©s mutuellement exclusives** et une seule Ã©preuve. La variable alÃ©atoire suit donc une **loi de Bernoulli** :

> [!NOTE]
> $$X_i \sim B(1 ; p) \quad \text{avec} \quad P(X_i = x_i) = p^{x_i} (1 - p)^{1 - x_i}$$

---

### Question 2 : DÃ©termination de l'EMV

On dispose d'un Ã©chantillon $(x_1, \ldots, x_n)$. L'estimateur par maximum de vraisemblance (EMV) de $p$, seul paramÃ¨tre de la loi de $X_i$ Ã  estimer, est basÃ© sur le principe suivant : on va chercher l'estimateur $\hat{p}$ qui rend la fonction de vraisemblance **maximale**.

#### Fonction de vraisemblance

$$L(x_1, \ldots, x_n ; p) = P(X = x_1) \times \cdots \times P(X = x_n)$$

#### Log-vraisemblance

$$\ln L(x_1, \ldots, x_n ; p) = \sum_{i=1}^n x_i \ln p + \left(n - \sum_{i=1}^n x_i\right) \ln (1 - p)$$

#### Calcul de la dÃ©rivÃ©e

$$\frac{\partial \ln L(\cdot)}{\partial p} = \frac{\sum_{i=1}^n x_i}{p} - \frac{n - \sum_{i=1}^n x_i}{1 - p}$$

#### Condition de premier ordre

$$\frac{\partial \ln L(\cdot)}{\partial p} = 0 \Rightarrow \hat{p} = \frac{\sum_{i=1}^n x_i}{n} = \bar{x}$$

> [!IMPORTANT]
> $$\boxed{\hat{p}(x_1, \ldots, x_n) = \bar{x}}$$

---

### Question 3 : PropriÃ©tÃ©s de l'estimateur

On suppose que les $X_i$ sont **indÃ©pendantes et identiquement distribuÃ©es**, cela signifie que pour tout $i = 1, \ldots, n$ : $X_i \equiv X$, donc :

- $E(X_i) = E(X) = p$ [puisque $X \sim B(1 ; p)$ : l'espÃ©rance d'une v.a. de Bernoulli est $p$]
- $V(X_i) = V(X) = p(1 - p)$ [la variance d'une v.a. de Bernoulli est $pq$]

---

#### PropriÃ©tÃ© 1 : $\hat{p}(X_1, \ldots, X_n)$ est un estimateur sans biais (ESB)

L'estimateur $\hat{p}(X_1, \ldots, X_n)$ est sans biais si son espÃ©rance est Ã©gale Ã  sa vraie valeur :

$$E[\hat{p}(X_1, \ldots, X_n)] = p$$

**DÃ©monstration :**

$$E[\hat{p}(X_1, \ldots, X_n)] = E\left[\frac{\sum_{i=1}^n X_i}{n}\right] = \frac{1}{n} \sum_{i=1}^n E[X_i] = \frac{1}{n} \cdot n p = p$$

âœ… L'estimateur est **sans biais**.

---

#### PropriÃ©tÃ© 2 : L'estimateur $\hat{p}(X_1, \ldots, X_n)$ est efficace

L'inÃ©galitÃ© de **FrÃ©chet-Rao-CramÃ©r-Darmois** indique que tout estimateur sans biais vÃ©rifie :

$$V(\hat{p}) \geq \frac{1}{I(p)}$$

avec $I(p)$ la quantitÃ© d'information de Fisher :

$$I(p) = -E\left[\frac{\partial^2 \ln L(\cdot)}{\partial p^2}\right]$$

La variance de l'estimateur est minimale si :

$$V(\hat{p}) = \frac{1}{I(p)}$$

**Calcul de $V(\hat{p})$ :**

$$V(\hat{p}(X_1, \ldots, X_n)) = V\left[\frac{\sum_{i=1}^n X_i}{n}\right] = \frac{1}{n^2} \sum_{i=1}^n V(X_i) = \frac{1}{n^2} \cdot n p(1 - p) = \frac{p(1 - p)}{n}$$

**Calcul de $I(p)$ :**

$$I(p) = -E\left[\frac{\partial^2 \ln L(\cdot)}{\partial p^2}\right] = -E\left[-\frac{\sum_{i=1}^n X_i}{p^2} - \frac{n - \sum_{i=1}^n X_i}{(1 - p)^2}\right] = \frac{n}{p(1 - p)}$$

**VÃ©rification :**

$$V(\hat{p}(X_1, \ldots, X_n)) = \frac{1}{I(p)}$$

> [!IMPORTANT]
> L'estimateur $\hat{p}(X_1, \ldots, X_n)$ est donc **efficace**.

---

#### PropriÃ©tÃ© 3 : L'estimateur $\hat{p}$ est convergent

$\hat{p}(X_1, \ldots, X_n)$ est un estimateur convergent s'il converge en limite de probabilitÃ© vers sa vraie valeur :

$$\lim_{n \to \infty} P(\hat{p} = p) = 1$$

Cela signifie que :

$$\forall \epsilon > 0, \exists \eta > 0 \text{ tel que } P(|\hat{p} - p| > \epsilon) < \eta$$

**DÃ©monstration :**

Comme $\hat{p}$ est un ESB, montrer qu'il est convergent revient Ã  montrer que lorsque la taille de l'Ã©chantillon augmente, la variance de l'estimateur tend vers 0 :

$$\lim_{n \to \infty} V(\hat{p}(X_1, \ldots, X_n)) = 0$$

On a :

$$\lim_{n \to \infty} V(\hat{p}(X_1, \ldots, X_n)) = \lim_{n \to \infty} \frac{p(1 - p)}{n} = 0$$

> [!IMPORTANT]
> L'estimateur est donc **convergent**. De plus, comme il est sans biais, il est **absolument convergent**.

---

## ğŸ“˜ Exercice 6 : Application du principe du maximum de vraisemblance Ã  la loi normale afin d'identifier un biais

### Ã‰noncÃ©

Soit $X$ la variable alÃ©atoire continue : Â« taille des individus Â» avec $X \sim N(m ; \sigma)$.

DÃ©terminez le(s) estimateur(s) du maximum de vraisemblance du (des) paramÃ¨tre(s) issu(s) de la loi de $X$.

> [!NOTE]
> **Objectifs de cet exercice :**
> 
> (i) Le maximum de vraisemblance peut permettre de trouver **plus d'un EMV**.
> 
> (ii) Il est possible qu'un EMV **ne remplisse pas** la condition minimale Â« sans biais Â».

---

### RÃ©solution

Si $X$ suit une loi normale, la densitÃ© de probabilitÃ© est donnÃ©e par :

$$f(x) = \frac{1}{\sigma \sqrt{2 \pi}} \exp \left(-\frac{1}{2} \left(\frac{x - m}{\sigma}\right)^2\right)$$

Les EMV de $\sigma$ et $m$, seuls paramÃ¨tres de la loi de $X$ Ã  estimer sont basÃ©s sur le principe suivant : on cherche les estimateurs $\hat{\sigma}$ et $\hat{m}$ qui rendent la fonction de vraisemblance maximale.

#### Fonction de vraisemblance

$$L(x_1, \ldots, x_n ; m ; \sigma) = f(x_1) \times \cdots \times f(x_n) = \left(\frac{1}{\sigma \sqrt{2 \pi}}\right)^n \exp \left(-\frac{1}{2} \sum_{i=1}^n \left(\frac{x_i - m}{\sigma}\right)^2\right)$$

#### Log-vraisemblance

$$\ln L(x_1, \ldots, x_n ; m ; \sigma) = -n \ln \sigma - n \ln[\sqrt{2 \pi}] - \frac{1}{2} \sum_{i=1}^n \left(\frac{x_i - m}{\sigma}\right)^2$$

#### EMV de $m$

$$\frac{\partial \ln L(\cdot)}{\partial m} = 0 \Rightarrow \left(-\frac{2}{2 \sigma^2} \sum_{i=1}^n (x_i - m)\right) = 0$$

D'oÃ¹ :

> [!IMPORTANT]
> $$\boxed{\hat{m} = \frac{1}{n} \sum_{i=1}^n x_i = \bar{x}}$$

#### EMV de $\sigma$

$$\frac{\partial \ln L(\cdot)}{\partial \sigma} = -\frac{n}{\sigma} + \frac{4 \delta \sum_{i=1}^n (x_i - m)^2}{4 \sigma^3} = 0$$

Alors :

> [!IMPORTANT]
> $$\boxed{\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \hat{m})^2 = s^2}$$

---

### PropriÃ©tÃ©s de $\hat{m}(X_1, \ldots, X_n)$

On peut dÃ©montrer que $\hat{m}(X_1, \ldots, X_n)$ satisfait **toutes les propriÃ©tÃ©s** :

| PropriÃ©tÃ© | DÃ©monstration |
|-----------|---------------|
| **Sans biais** | $E[\hat{m}(X_1, \ldots, X_n)] = m$ |
| **Efficace** | $I(m) = 1/V[\hat{m}(X_1, \ldots, X_n)] = n/\sigma^2$ |
| **Convergent** | $\lim_{n \to \infty} V[\hat{m}(X_1, \ldots, X_n)] = \lim_{n \to \infty} \frac{\sigma^2}{n} = 0$ |
| **Exhaustif** | $m^* = c \frac{\sum_{i=1}^n \alpha(X_i)}{n} = c \bar{X}$. Pour $c = 1$, $\hat{m}(X_1, \ldots, X_n)$ appartient Ã  la famille $m^*$ |

---

### Le problÃ¨me du biais de la variance empirique $S^2$

> [!WARNING]
> On peut dÃ©montrer que la **variance empirique $S^2$ est biaisÃ©e** !

En effet :

$$E(S^2) = \sigma^2 - \frac{\sigma^2}{n}$$

Le biais est donc $-\frac{\sigma^2}{n}$.

On effectue une mise en facteur :

$$E(S^2) = \sigma^2 - \frac{\sigma^2}{n} = \sigma^2 \left(\frac{n - 1}{n}\right)$$

#### Correction du biais

Pour neutraliser le biais, il suffit de multiplier la variance $S^2$ par l'inverse du terme entre parenthÃ¨ses afin de trouver une **variance non biaisÃ©e** (variance empirique corrigÃ©e du biais que l'on notera $\hat{S}^2$) :

> [!IMPORTANT]
> $$\boxed{\hat{S}^2 = S^2 \left(\frac{n}{n - 1}\right) = \frac{1}{n - 1} \sum_{i=1}^n (X_i - \hat{m})^2}$$

---

## ğŸ’¡ Conseils Pratiques

> [!TIP]
> **Conseil 1 : Ã‰chantillon empirique vs thÃ©orique**
> 
> La mÃ©thode du maximum de vraisemblance s'applique Ã  l'**Ã©chantillon empirique** $(x_1, \ldots, x_n)$. Les propriÃ©tÃ©s de l'estimateur ainsi trouvÃ© sont vÃ©rifiÃ©es sur l'**Ã©chantillon thÃ©orique** $(X_1, \ldots, X_n)$.

> [!TIP]
> **Conseil 2 : Que faire si un estimateur est biaisÃ© ?**
> 
> Si $E[\hat{\theta}(X_1, \ldots, X_n)] = k \theta$, alors le biais est neutralisÃ© en concevant un nouvel ESB $\tilde{\theta}$ tel que :
> 
> $$\tilde{\theta} = \frac{1}{k} \hat{\theta}$$

---

## ğŸ“Š Tableau rÃ©capitulatif des rÃ©sultats

| Exercice | Loi | ParamÃ¨tre | Estimateur | Sans biais | Efficace | Convergent |
|----------|-----|-----------|------------|------------|----------|------------|
| 1 | $B(10; p)$ | $p$ | $\hat{p} = \frac{\bar{K}}{10}$ | âœ… | âœ… | âœ… |
| 2 | $f(x) = \frac{1}{\theta}e^{-x/\theta}$ | $\theta$ | $\hat{\theta} = \bar{x}$ | âœ… | âœ… | âœ… |
| 4 | $f(x) = \frac{2}{\theta}xe^{-x^2/\theta}$ | $\theta$ | $\hat{\theta} = \overline{x^2}$ | âœ… | âœ… | âœ… |
| 5 | $B(1; p)$ | $p$ | $\hat{p} = \bar{x}$ | âœ… | âœ… | âœ… |
| 6 | $N(m; \sigma)$ | $m$ | $\hat{m} = \bar{x}$ | âœ… | âœ… | âœ… |
| 6 | $N(m; \sigma)$ | $\sigma^2$ | $S^2 = \frac{1}{n}\sum(x_i - \bar{x})^2$ | âŒ | â€” | â€” |
| 6 | $N(m; \sigma)$ | $\sigma^2$ | $\hat{S}^2 = \frac{1}{n-1}\sum(x_i - \bar{x})^2$ | âœ… | â€” | â€” |

---

*Chapitre 9 â€” Estimation Ponctuelle*
